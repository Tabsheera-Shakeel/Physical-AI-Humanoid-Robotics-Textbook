"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[4432],{4166:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>i,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"capstone-project/implementation-steps","title":"Implementation Steps: Building Your Autonomous Assistant","description":"Now that we have a clear system design, let\'s dive into the implementation details. This section will provide a step-by-step guide to building each component of our autonomous household assistant.","source":"@site/docs/capstone-project/implementation-steps.mdx","sourceDirName":"capstone-project","slug":"/capstone-project/implementation-steps","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/capstone-project/implementation-steps","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"sidebar_label":"Implementation Steps"},"sidebar":"mainSidebar","previous":{"title":"System Design and Integration","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/capstone-project/system-design-integration"},"next":{"title":"Testing and Validation","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/capstone-project/testing-validation"}}');var s=t(4848),a=t(8453);const i={sidebar_label:"Implementation Steps"},r="Implementation Steps: Building Your Autonomous Assistant",c={},l=[{value:"1. Setting Up Your ROS 2 Workspace",id:"1-setting-up-your-ros-2-workspace",level:2},{value:"2. Implementing the NLU Pipeline",id:"2-implementing-the-nlu-pipeline",level:2},{value:"Speech-to-Text (STT)",id:"speech-to-text-stt",level:3},{value:"Intent and Entity Recognition",id:"intent-and-entity-recognition",level:3},{value:"3. Perception Node (Object Detection)",id:"3-perception-node-object-detection",level:2},{value:"4. Task Planner and Action Generation",id:"4-task-planner-and-action-generation",level:2},{value:"5. Robot Control (Navigation and Manipulation)",id:"5-robot-control-navigation-and-manipulation",level:2},{value:"Navigation (Nav2)",id:"navigation-nav2",level:3},{value:"6. Update <code>setup.py</code>",id:"6-update-setuppy",level:2},{value:"7. Launch File",id:"7-launch-file",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"implementation-steps-building-your-autonomous-assistant",children:"Implementation Steps: Building Your Autonomous Assistant"})}),"\n",(0,s.jsx)(n.p,{children:"Now that we have a clear system design, let's dive into the implementation details. This section will provide a step-by-step guide to building each component of our autonomous household assistant."}),"\n",(0,s.jsx)(n.h2,{id:"1-setting-up-your-ros-2-workspace",children:"1. Setting Up Your ROS 2 Workspace"}),"\n",(0,s.jsx)(n.p,{children:"Ensure your ROS 2 workspace is properly set up and sourced. If you haven't already, create a new package for your capstone project:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws/src\nros2 pkg create --build-type ament_python capstone_robot\n"})}),"\n",(0,s.jsx)(n.h2,{id:"2-implementing-the-nlu-pipeline",children:"2. Implementing the NLU Pipeline"}),"\n",(0,s.jsx)(n.p,{children:"The NLU pipeline will take a voice command, convert it to text, and then extract the intent and entities."}),"\n",(0,s.jsx)(n.h3,{id:"speech-to-text-stt",children:"Speech-to-Text (STT)"}),"\n",(0,s.jsx)(n.p,{children:"For simplicity, we will assume a pre-existing STT service or use a simple Python library."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# In capstone_robot/capstone_robot/stt_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\n\nclass STTNode(Node):\n    def __init__(self):\n        super().__init__('stt_node')\n        self.publisher_ = self.create_publisher(String, 'voice_command_text', 10)\n        self.get_logger().info('STT Node ready. Waiting for voice input...')\n        # Simulate voice input for demonstration\n        self.timer = self.create_timer(5.0, self.simulate_voice_input)\n\n    def simulate_voice_input(self):\n        # In a real scenario, this would come from a microphone\n        command = \"pick up the water bottle from the table and put it in the fridge\"\n        msg = String()\n        msg.data = command\n        self.publisher_.publish(msg)\n        self.get_logger().info(f'Simulated voice command: \"{command}\"')\n        self.timer.cancel() # Only simulate once for this example\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = STTNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"intent-and-entity-recognition",children:"Intent and Entity Recognition"}),"\n",(0,s.jsx)(n.p,{children:"We will use a simple rule-based approach or a pre-trained model (e.g., from Hugging Face) to extract intent and entities."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# In capstone_robot/capstone_robot/nlu_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport json\n\nclass NLUNode(Node):\n    def __init__(self):\n        super().__init__(\'nlu_node\')\n        self.subscription = self.create_subscription(\n            String,\n            \'voice_command_text\',\n            self.command_callback,\n            10)\n        self.publisher_ = self.create_publisher(String, \'parsed_command\', 10)\n        self.get_logger().info(\'NLU Node ready.\')\n\n    def command_callback(self, msg):\n        command_text = msg.data\n        self.get_logger().info(f\'Received command: "{command_text}"\')\n\n        # Simple rule-based parsing for demonstration\n        intent = "pick_and_place"\n        object_name = "water_bottle"\n        source_location = "table"\n        destination_location = "fridge"\n\n        parsed_command = {\n            "intent": intent,\n            "object": object_name,\n            "source": source_location,\n            "destination": destination_location\n        }\n        \n        parsed_msg = String()\n        parsed_msg.data = json.dumps(parsed_command)\n        self.publisher_.publish(parsed_msg)\n        self.get_logger().info(f\'Parsed command: {parsed_msg.data}\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = NLUNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"3-perception-node-object-detection",children:"3. Perception Node (Object Detection)"}),"\n",(0,s.jsx)(n.p,{children:"The perception node will use the robot's camera feed to detect objects and their 3D poses. We will integrate with an Isaac ROS object detection pipeline."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# This node would typically wrap Isaac ROS object detection\n# In capstone_robot/capstone_robot/perception_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import String # For detected objects info\nimport json\n\nclass PerceptionNode(Node):\n    def __init__(self):\n        super().__init__(\'perception_node\')\n        self.image_subscription = self.create_subscription(\n            Image,\n            \'/camera/image_raw\', # Assuming camera publishes here\n            self.image_callback,\n            10)\n        self.object_publisher = self.create_publisher(String, \'detected_objects\', 10)\n        self.get_logger().info(\'Perception Node ready.\')\n\n    def image_callback(self, msg):\n        # In a real scenario, this would process the image with an object detection model\n        # For now, simulate detection of water bottle, table, fridge\n        detected_objects_info = {\n            "water_bottle": {"pose": {"x": 0.5, "y": 0.1, "z": 0.8}, "frame_id": "map"},\n            "table": {"pose": {"x": 0.6, "y": 0.0, "z": 0.7}, "frame_id": "map"},\n            "fridge": {"pose": {"x": -1.0, "y": 1.5, "z": 1.0}, "frame_id": "map"}\n        }\n        obj_msg = String()\n        obj_msg.data = json.dumps(detected_objects_info)\n        self.object_publisher.publish(obj_msg)\n        # self.get_logger().info(f\'Published detected objects: {obj_msg.data}\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = PerceptionNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"4-task-planner-and-action-generation",children:"4. Task Planner and Action Generation"}),"\n",(0,s.jsx)(n.p,{children:"This is the core intelligence that orchestrates the robot's behavior."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# In capstone_robot/capstone_robot/task_planner_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport json\nimport time\n\nclass TaskPlannerNode(Node):\n    def __init__(self):\n        super().__init__(\'task_planner_node\')\n        self.command_subscription = self.create_subscription(\n            String,\n            \'parsed_command\',\n            self.command_callback,\n            10)\n        self.objects_subscription = self.create_subscription(\n            String,\n            \'detected_objects\',\n            self.objects_callback,\n            10)\n        self.action_publisher = self.create_publisher(String, \'robot_actions\', 10)\n        self.get_logger().info(\'Task Planner Node ready.\')\n\n        self.current_parsed_command = None\n        self.current_detected_objects = {}\n        self.task_in_progress = False\n\n    def command_callback(self, msg):\n        self.current_parsed_command = json.loads(msg.data)\n        self.get_logger().info(f\'Task Planner received command: {self.current_parsed_command}\')\n        if not self.task_in_progress:\n            self.execute_task()\n\n    def objects_callback(self, msg):\n        self.current_detected_objects = json.loads(msg.data)\n        # self.get_logger().info(f\'Task Planner received objects: {self.current_detected_objects}\')\n        if self.current_parsed_command and not self.task_in_progress:\n            self.execute_task()\n\n    def execute_task(self):\n        if not self.current_parsed_command or not self.current_detected_objects:\n            return\n\n        self.task_in_progress = True\n        intent = self.current_parsed_command["intent"]\n        obj = self.current_parsed_command["object"]\n        src = self.current_parsed_command["source"]\n        dest = self.current_parsed_command["destination"]\n\n        self.get_logger().info(f"Executing task: {intent} {obj} from {src} to {dest}")\n\n        # Simulate actions\n        actions = []\n        \n        # Navigate to source\n        if src in self.current_detected_objects:\n            src_pose = self.current_detected_objects[src]["pose"]\n            actions.append({"type": "navigate", "target_pose": src_pose})\n        \n        # Pick object\n        if obj in self.current_detected_objects:\n            obj_pose = self.current_detected_objects[obj]["pose"]\n            actions.append({"type": "pick", "target_object_pose": obj_pose})\n        \n        # Navigate to destination\n        if dest in self.current_detected_objects:\n            dest_pose = self.current_detected_objects[dest]["pose"]\n            actions.append({"type": "navigate", "target_pose": dest_pose})\n        \n        # Place object\n        actions.append({"type": "place", "target_location": dest_pose})\n\n        for action in actions:\n            action_msg = String()\n            action_msg.data = json.dumps(action)\n            self.action_publisher.publish(action_msg)\n            self.get_logger().info(f"Published action: {action_msg.data}")\n            time.sleep(2) # Simulate action duration\n\n        self.get_logger().info("Task completed!")\n        self.task_in_progress = False\n        self.current_parsed_command = None # Reset for next command\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = TaskPlannerNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"5-robot-control-navigation-and-manipulation",children:"5. Robot Control (Navigation and Manipulation)"}),"\n",(0,s.jsx)(n.p,{children:"These nodes will interface with the simulated robot."}),"\n",(0,s.jsx)(n.h3,{id:"navigation-nav2",children:"Navigation (Nav2)"}),"\n",(0,s.jsxs)(n.p,{children:["We will use the Nav2 stack. The ",(0,s.jsx)(n.code,{children:"navigate"})," action from the task planner will be sent to Nav2."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# This would involve calling Nav2 actions\n# For simulation, we assume Nav2 is running and receives goals\n# In capstone_robot/capstone_robot/robot_controller_node.py (simplified)\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport json\n\nclass RobotControllerNode(Node):\n    def __init__(self):\n        super().__init__(\'robot_controller_node\')\n        self.action_subscription = self.create_subscription(\n            String,\n            \'robot_actions\',\n            self.action_callback,\n            10)\n        self.get_logger().info(\'Robot Controller Node ready.\')\n\n    def action_callback(self, msg):\n        action = json.loads(msg.data)\n        action_type = action["type"]\n        self.get_logger().info(f"Executing robot action: {action_type}")\n\n        if action_type == "navigate":\n            target_pose = action["target_pose"]\n            self.get_logger().info(f"Navigating to: {target_pose}")\n            # Here you would send a goal to Nav2\n        elif action_type == "pick":\n            target_object_pose = action["target_object_pose"]\n            self.get_logger().info(f"Picking object at: {target_object_pose}")\n            # Here you would send commands to MoveIt for grasping\n        elif action_type == "place":\n            target_location = action["target_location"]\n            self.get_logger().info(f"Placing object at: {target_location}")\n            # Here you would send commands to MoveIt for placing\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = RobotControllerNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsxs)(n.h2,{id:"6-update-setuppy",children:["6. Update ",(0,s.jsx)(n.code,{children:"setup.py"})]}),"\n",(0,s.jsxs)(n.p,{children:["Add the new nodes to your ",(0,s.jsx)(n.code,{children:"capstone_robot/setup.py"})," file:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"entry_points={\n    'console_scripts': [\n        'stt_node = capstone_robot.stt_node:main',\n        'nlu_node = capstone_robot.nlu_node:main',\n        'perception_node = capstone_robot.perception_node:main',\n        'task_planner_node = capstone_robot.task_planner_node:main',\n        'robot_controller_node = capstone_robot.robot_controller_node:main',\n    ],\n},\n"})}),"\n",(0,s.jsx)(n.h2,{id:"7-launch-file",children:"7. Launch File"}),"\n",(0,s.jsx)(n.p,{children:"Finally, create a launch file to bring up all the components."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# In capstone_robot/launch/capstone_launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        Node(\n            package='capstone_robot',\n            executable='stt_node',\n            name='stt_node',\n            output='screen'\n        ),\n        Node(\n            package='capstone_robot',\n            executable='nlu_node',\n            name='nlu_node',\n            output='screen'\n        ),\n        Node(\n            package='capstone_robot',\n            executable='perception_node',\n            name='perception_node',\n            output='screen'\n        ),\n        Node(\n            package='capstone_robot',\n            executable='task_planner_node',\n            name='task_planner_node',\n            output='screen'\n        ),\n        Node(\n            package='capstone_robot',\n            executable='robot_controller_node',\n            name='robot_controller_node',\n            output='screen'\n        ),\n    ])\n"})}),"\n",(0,s.jsx)(n.p,{children:"This section provides a high-level overview of the implementation. Each of these components would involve significant development, especially the integration with Nav2 and MoveIt 2, which are complex frameworks in themselves."})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>r});var o=t(6540);const s={},a=o.createContext(s);function i(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);