"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[4576],{5368:(n,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>s,default:()=>p,frontMatter:()=>r,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"module-3-ai-robot-brain/navigation-path-planning-isaac","title":"navigation-path-planning-isaac","description":"","source":"@site/docs/module-3-ai-robot-brain/navigation-path-planning-isaac.mdx","sourceDirName":"module-3-ai-robot-brain","slug":"/module-3-ai-robot-brain/navigation-path-planning-isaac","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/module-3-ai-robot-brain/navigation-path-planning-isaac","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"mainSidebar","previous":{"title":"pose-estimation-tracking","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/module-3-ai-robot-brain/pose-estimation-tracking"},"next":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/category/module-4-vision-language-action-vla"}}');var i=a(4848),e=a(8453);const r={},s=void 0,c={},l=[];function u(n){return(0,i.jsx)(i.Fragment,{})}function p(n={}){const{wrapper:t}={...(0,e.R)(),...n.components};return t?(0,i.jsx)(t,{...n,children:(0,i.jsx)(u,{...n})}):u()}},8453:(n,t,a)=>{a.d(t,{R:()=>r,x:()=>s});var o=a(6540);const i={},e=o.createContext(i);function r(n){const t=o.useContext(e);return o.useMemo(function(){return"function"==typeof n?n(t):{...t,...n}},[t,n])}function s(n){let t;return t=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:r(n.components),o.createElement(e.Provider,{value:t},n.children)}}}]);