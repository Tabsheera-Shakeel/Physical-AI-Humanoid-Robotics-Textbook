"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[4431],{8529:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"mainSidebar":[{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/introduction/what-is-physical-ai","label":"What Is Physical AI?","docId":"introduction/what-is-physical-ai","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/introduction/why-humanoid-robots","label":"Why Humanoid Robots?","docId":"introduction/why-humanoid-robots","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/introduction/book-overview","label":"Book Overview","docId":"introduction/book-overview","unlisted":false},{"type":"category","label":"Module 1: ROS 2 Fundamentals","items":[{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/module-1-ros2/ros2-concepts","label":"Core ROS 2 Concepts","docId":"module-1-ros2/ros2-concepts","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/module-1-ros2/ros2-setup","label":"Setting Up ROS 2","docId":"module-1-ros2/ros2-setup","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/module-1-ros2/basic-ros2-programming","label":"Basic ROS 2 Programming","docId":"module-1-ros2/basic-ros2-programming","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/module-1-ros2/urdf-xacro","label":"URDF and Xacro","docId":"module-1-ros2/urdf-xacro","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/Physical-AI-Humanoid-Robotics-Textbook/category/module-1-ros-2-fundamentals"},{"type":"category","label":"Module 2: Digital Twin Simulation","items":[{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/module-2-digital-twin/intro-gazebo-unity","label":"Intro to Gazebo and Unity","docId":"module-2-digital-twin/intro-gazebo-unity","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/module-2-digital-twin/robot-models-simulation","label":"Robot Models in Simulation","docId":"module-2-digital-twin/robot-models-simulation","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/module-2-digital-twin/simulating-sensors-actuators","label":"Simulating Sensors and Actuators","docId":"module-2-digital-twin/simulating-sensors-actuators","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/module-2-digital-twin/ros2-integration-simulation","label":"ROS 2 Integration with Simulation","docId":"module-2-digital-twin/ros2-integration-simulation","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/Physical-AI-Humanoid-Robotics-Textbook/category/module-2-digital-twin-simulation"},{"type":"category","label":"Module 3: AI-Robot Brain (NVIDIA Isaac)","items":[{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/module-3-ai-robot-brain/isaac-ecosystem","label":"The NVIDIA Isaac Ecosystem","docId":"module-3-ai-robot-brain/isaac-ecosystem","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/module-3-ai-robot-brain/setup-isaac","label":"Setting Up NVIDIA Isaac","docId":"module-3-ai-robot-brain/setup-isaac","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/module-3-ai-robot-brain/object-detection-isaac","label":"Object Detection with Isaac","docId":"module-3-ai-robot-brain/object-detection-isaac","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/module-3-ai-robot-brain/pose-estimation-tracking","label":"Pose Estimation and Tracking","docId":"module-3-ai-robot-brain/pose-estimation-tracking","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/module-3-ai-robot-brain/navigation-path-planning-isaac","label":"Navigation and Path Planning","docId":"module-3-ai-robot-brain/navigation-path-planning-isaac","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/Physical-AI-Humanoid-Robotics-Textbook/category/module-3-ai-robot-brain-nvidia-isaac"},{"type":"category","label":"Module 4: Vision-Language-Action (VLA)","items":[{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/module-4-vla/intro-vla","label":"Introduction to VLA","docId":"module-4-vla/intro-vla","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/module-4-vla/natural-language-understanding","label":"Natural Language Understanding","docId":"module-4-vla/natural-language-understanding","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/module-4-vla/action-generation","label":"Action Generation","docId":"module-4-vla/action-generation","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/module-4-vla/vla-frameworks-apis","label":"VLA Frameworks and APIs","docId":"module-4-vla/vla-frameworks-apis","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/module-4-vla/ethical-considerations","label":"Ethical Considerations","docId":"module-4-vla/ethical-considerations","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/Physical-AI-Humanoid-Robotics-Textbook/category/module-4-vision-language-action-vla"},{"type":"category","label":"Capstone Project","items":[{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/capstone-project/project-overview","label":"Project Overview","docId":"capstone-project/project-overview","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/capstone-project/system-design-integration","label":"System Design and Integration","docId":"capstone-project/system-design-integration","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/capstone-project/implementation-steps","label":"Implementation Steps","docId":"capstone-project/implementation-steps","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/capstone-project/testing-validation","label":"Testing and Validation","docId":"capstone-project/testing-validation","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/capstone-project/future-work","label":"Future Work and Extensions","docId":"capstone-project/future-work","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/Physical-AI-Humanoid-Robotics-Textbook/category/capstone-project"},{"type":"category","label":"Appendices","items":[{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/appendices/ros2-cheatsheet","label":"ROS 2 Cheatsheet","docId":"appendices/ros2-cheatsheet","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/appendices/isaac-api-reference","label":"Isaac API Reference","docId":"appendices/isaac-api-reference","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/appendices/troubleshooting-guide","label":"Troubleshooting Guide","docId":"appendices/troubleshooting-guide","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Textbook/appendices/glossary","label":"Glossary","docId":"appendices/glossary","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/Physical-AI-Humanoid-Robotics-Textbook/category/appendices"}]},"docs":{"appendices/assessments-grading":{"id":"appendices/assessments-grading","title":"Assessments & Grading","description":"This section will detail the assessment methods, grading criteria, and expectations for successful completion of the course material."},"appendices/glossary":{"id":"appendices/glossary","title":"Glossary of Key Terms","description":"This glossary provides definitions for important terms and concepts used throughout this book, covering ROS 2, NVIDIA Isaac, Physical AI, and general robotics and AI terminology.","sidebar":"mainSidebar"},"appendices/hardware-requirements-2025-edition":{"id":"appendices/hardware-requirements-2025-edition","title":"Hardware Requirements (2025 Edition)","description":"This section will detail the recommended hardware specifications for successfully completing the practical exercises and projects in the book."},"appendices/isaac-api-reference":{"id":"appendices/isaac-api-reference","title":"NVIDIA Isaac API Reference: A Guide to Key Components","description":"The NVIDIA Isaac platform is vast, encompassing a wide array of tools, libraries, and APIs for robotics development. This section provides a high-level overview of the most important API components within the Isaac ecosystem and guides you to their official documentation for in-depth reference.","sidebar":"mainSidebar"},"appendices/learning-outcomes":{"id":"appendices/learning-outcomes","title":"Learning Outcomes","description":"This section will outline the key learning objectives and expected outcomes for each module of the book."},"appendices/physical-ai-lab-architecture":{"id":"appendices/physical-ai-lab-architecture","title":"Physical AI Lab Architecture","description":"This section will describe the proposed architecture for a Physical AI Lab, including hardware, software, and networking considerations."},"appendices/ros2-cheatsheet":{"id":"appendices/ros2-cheatsheet","title":"ROS 2 Cheatsheet: Essential Commands and Concepts","description":"This cheatsheet provides a quick reference for the most commonly used ROS 2 commands, concepts, and development practices. It\'s designed to be a handy guide for both beginners and experienced ROS 2 developers.","sidebar":"mainSidebar"},"appendices/troubleshooting-guide":{"id":"appendices/troubleshooting-guide","title":"Troubleshooting Guide: Common Issues in ROS 2 and Isaac Development","description":"Developing with ROS 2 and NVIDIA Isaac can sometimes present challenges. This troubleshooting guide aims to help you diagnose and resolve common issues you might encounter during installation, development, and simulation.","sidebar":"mainSidebar"},"appendices/week-detailed-schedule":{"id":"appendices/week-detailed-schedule","title":"13-Week Detailed Schedule","description":"This section will provide a detailed breakdown of the book\'s content across a 13-week study schedule, including topics, readings, and exercises."},"capstone-project/future-work":{"id":"capstone-project/future-work","title":"Future Work and Extensions: Beyond the Capstone Project","description":"Our capstone project provides a solid foundation for building an autonomous household assistant. However, the field of Physical AI is rapidly evolving, and there are countless opportunities to extend and improve upon this project. This section explores some exciting avenues for future work and research.","sidebar":"mainSidebar"},"capstone-project/implementation-steps":{"id":"capstone-project/implementation-steps","title":"Implementation Steps: Building Your Autonomous Assistant","description":"Now that we have a clear system design, let\'s dive into the implementation details. This section will provide a step-by-step guide to building each component of our autonomous household assistant.","sidebar":"mainSidebar"},"capstone-project/project-overview":{"id":"capstone-project/project-overview","title":"Capstone Project: The Autonomous Household Assistant","description":"Welcome to the capstone project! This is where we will bring together all the concepts, tools, and techniques you have learned throughout this book to build a complete, end-to-end Physical AI system.","sidebar":"mainSidebar"},"capstone-project/system-design-integration":{"id":"capstone-project/system-design-integration","title":"System Design and Integration","description":"A complex robotics application like our capstone project requires a well-designed software architecture. In this section, we will lay out the system design for our autonomous household assistant, detailing the various components and how they are integrated.","sidebar":"mainSidebar"},"capstone-project/testing-validation":{"id":"capstone-project/testing-validation","title":"Testing and Validation: Ensuring Robustness","description":"Building a complex autonomous system like our household assistant requires rigorous testing and validation. This section will outline the strategies and tools we can use to ensure our robot operates safely, reliably, and effectively.","sidebar":"mainSidebar"},"intro":{"id":"intro","title":"Introduction to Physical AI & Humanoid Robotics","description":"Welcome to the Physical AI and Humanoid Robotics textbook and documentation portal."},"introduction/book-overview":{"id":"introduction/book-overview","title":"Book Overview: Your Journey to Building Autonomous Humanoids","description":"Forget theoretical surveys. This book is your definitive, battle-tested curriculum to transform you from a novice into a skilled practitioner capable of building and controlling a fully autonomous, voice-controlled humanoid robot. Whether in high-fidelity simulation or on real hardware, you will master the cutting-edge technologies driving the Physical AI revolution.","sidebar":"mainSidebar"},"introduction/intro":{"id":"introduction/intro","title":"Introduction","description":""},"introduction/Table-of-contents":{"id":"introduction/Table-of-contents","title":"**Table of Contents**","description":"1. What Is Physical AI"},"introduction/what-is-physical-ai":{"id":"introduction/what-is-physical-ai","title":"What Is Physical AI? Bridging the Digital and Physical Worlds","description":"For decades, Artificial Intelligence has captivated our imaginations, evolving from theoretical concepts to powerful digital entities. It has conquered complex games, generated stunning art, and mastered the nuances of human language, as evidenced by the remarkable capabilities of models like ChatGPT, Gemini, and LLaMA. This \\"disembodied\\" AI operates with unparalleled cognitive prowess within the digital realm.","sidebar":"mainSidebar"},"introduction/why-humanoid-robots":{"id":"introduction/why-humanoid-robots","title":"Why Humanoid Robots? The Embodiment of General-Purpose AI","description":"When envisioning the future of robotics, the image of a humanoid robot\u2014a machine with a head, a torso, two arms, and two legs\u2014is almost universally recognized. This isn\'t merely a product of science fiction; it represents a profound engineering and economic imperative. The humanoid form factor is rapidly emerging as the optimal solution for deploying general-purpose Physical AI in environments meticulously crafted for human beings.","sidebar":"mainSidebar"},"module-1-ros2/basic-ros2-programming":{"id":"module-1-ros2/basic-ros2-programming","title":"Basic ROS 2 Programming with Python","description":"Now that you have a solid understanding of the core ROS 2 concepts and have your environment set up, it\'s time to write your first ROS 2 program! We will create a simple publisher and subscriber in Python.","sidebar":"mainSidebar"},"module-1-ros2/index":{"id":"module-1-ros2/index","title":"Module 1: ROS 2 Fundamentalsss","description":""},"module-1-ros2/launch-params":{"id":"module-1-ros2/launch-params","title":"Launch Parameters","description":""},"module-1-ros2/ros2-concepts":{"id":"module-1-ros2/ros2-concepts","title":"Core ROS 2 Concepts","description":"Welcome to the world of ROS 2! ROS (Robot Operating System) is the middleware that has become the de facto standard for robotics development worldwide. It\'s not a traditional operating system, but rather a flexible framework of software libraries and tools that help you build complex robot applications.","sidebar":"mainSidebar"},"module-1-ros2/ros2-programming":{"id":"module-1-ros2/ros2-programming","title":"ROS 2 Programming","description":""},"module-1-ros2/ros2-setup":{"id":"module-1-ros2/ros2-setup","title":"Setting Up Your ROS 2 Environment","description":"Getting your development environment set up correctly is the first and most crucial step to starting your journey with ROS 2. This guide will walk you through installing ROS 2, creating a workspace, and introducing you to the essential command-line tools.","sidebar":"mainSidebar"},"module-1-ros2/setup-ros2":{"id":"module-1-ros2/setup-ros2","title":"setup-ros2","description":""},"module-1-ros2/urdf-xacro":{"id":"module-1-ros2/urdf-xacro","title":"Describing Your Robot with URDF and Xacro","description":"To work with a robot in ROS 2, especially in a simulator, you need a way to describe its physical structure. This is where the Unified Robot Description Format (URDF) comes in.","sidebar":"mainSidebar"},"module-2-digital-twin/gazebo-unity-intro":{"id":"module-2-digital-twin/gazebo-unity-intro","title":"Gazebo and Unity Introduction","description":""},"module-2-digital-twin/index":{"id":"module-2-digital-twin/index","title":"Module 2: Digital Twin Simulation","description":""},"module-2-digital-twin/intro-gazebo-unity":{"id":"module-2-digital-twin/intro-gazebo-unity","title":"Introduction to Robotics Simulation: Gazebo and Unity","description":"Simulation is an indispensable tool in modern robotics. It allows you to design, test, and validate your robot\'s hardware and software in a virtual environment before deploying it in the real world. This saves time, reduces costs, and minimizes the risk of damaging expensive hardware.","sidebar":"mainSidebar"},"module-2-digital-twin/robot-models":{"id":"module-2-digital-twin/robot-models","title":"Robot Models","description":""},"module-2-digital-twin/robot-models-simulation":{"id":"module-2-digital-twin/robot-models-simulation","title":"Creating and Using Robot Models for Simulation","description":"To bring your robot into a simulated world, you need a detailed model that the simulator can understand. As we learned in Module 1, the standard format for this in the ROS ecosystem is URDF (Unified Robot Description Format), often written with Xacro for simplicity.","sidebar":"mainSidebar"},"module-2-digital-twin/ros2-integration-simulation":{"id":"module-2-digital-twin/ros2-integration-simulation","title":"Integrating ROS 2 with Your Simulation","description":"Having a simulated robot that you can control with ROS 2 is a powerful combination. The key to making this work smoothly is the integration between the ROS 2 ecosystem and the simulator. This is typically achieved through a ROS bridge and managed with ROS 2 launch files.","sidebar":"mainSidebar"},"module-2-digital-twin/ros2-simulation-integration":{"id":"module-2-digital-twin/ros2-simulation-integration","title":"ROS 2 Simulation Integration","description":""},"module-2-digital-twin/sensors-actuators":{"id":"module-2-digital-twin/sensors-actuators","title":"Sensors and Actuators","description":""},"module-2-digital-twin/simulating-sensors-actuators":{"id":"module-2-digital-twin/simulating-sensors-actuators","title":"Simulating Sensors and Actuators in Gazebo","description":"A simulated robot is not very useful unless it can sense its environment and act upon it. In Gazebo, this is achieved through plugins. Plugins are shared libraries that are loaded at runtime and can interact with the simulation.","sidebar":"mainSidebar"},"module-3-ai-robot-brain/index":{"id":"module-3-ai-robot-brain/index","title":"Module 3: AI-Robot Brain","description":""},"module-3-ai-robot-brain/isaac-ecosystem":{"id":"module-3-ai-robot-brain/isaac-ecosystem","title":"The NVIDIA Isaac Ecosystem: A Comprehensive Overview","description":"NVIDIA Isaac is a powerful and comprehensive platform designed to accelerate the development and deployment of AI-powered robots. It provides a rich set of tools, libraries, and simulation environments that streamline the entire robotics workflow, from perception and navigation to manipulation and control. The ecosystem is built on NVIDIA\'s expertise in AI and GPU computing, offering high-performance solutions for a wide range of robotics applications.","sidebar":"mainSidebar"},"module-3-ai-robot-brain/isaac-setup":{"id":"module-3-ai-robot-brain/isaac-setup","title":"NVIDIA Isaac Setup","description":""},"module-3-ai-robot-brain/navigation-path-planning-isaac":{"id":"module-3-ai-robot-brain/navigation-path-planning-isaac","title":"Navigation and Path Planning with NVIDIA Isaac","description":"For a mobile robot to be autonomous, it must be able to navigate its environment safely and efficiently. This involves three core capabilities:","sidebar":"mainSidebar"},"module-3-ai-robot-brain/object-detection":{"id":"module-3-ai-robot-brain/object-detection","title":"Object Detection","description":""},"module-3-ai-robot-brain/object-detection-isaac":{"id":"module-3-ai-robot-brain/object-detection-isaac","title":"Object Detection with NVIDIA Isaac","description":"Object detection is a fundamental capability for any intelligent robot. It allows the robot to perceive and understand its environment by identifying and locating objects of interest. NVIDIA Isaac provides a powerful set of tools for implementing and accelerating object detection pipelines, from generating synthetic data for training to deploying hardware-accelerated models on the robot.","sidebar":"mainSidebar"},"module-3-ai-robot-brain/pose-estimation-tracking":{"id":"module-3-ai-robot-brain/pose-estimation-tracking","title":"Pose Estimation and Tracking with NVIDIA Isaac","description":"While object detection tells you what objects are in the environment, pose estimation tells you where they are in 3D space\u2014specifically, their position and orientation relative to the robot. This is a critical capability for any robot that needs to interact with its environment, such as a robotic arm that needs to grasp an object.","sidebar":"mainSidebar"},"module-3-ai-robot-brain/setup-isaac":{"id":"module-3-ai-robot-brain/setup-isaac","title":"Setting Up Your NVIDIA Isaac Development Environment","description":"Before you can start developing with the NVIDIA Isaac platform, you need to set up a suitable development environment. This involves ensuring you have the right hardware, installing the necessary software, and configuring your workspace. This guide will walk you through the process step by step.","sidebar":"mainSidebar"},"module-4-vla/action-generation":{"id":"module-4-vla/action-generation","title":"Action Generation in VLA Models","description":"Once a robot has understood a command through Natural Language Understanding (NLU), the next step is to figure out how to carry out that command. This is the \\"Action\\" part of a Vision-Language-Action (VLA) model, and it\'s where the robot\'s intelligence truly comes to life.","sidebar":"mainSidebar"},"module-4-vla/ethical-considerations":{"id":"module-4-vla/ethical-considerations","title":"Ethical Considerations for VLA Models","description":"The development of Vision-Language-Action (VLA) models represents a major leap forward in artificial intelligence and robotics. As we create robots that can understand our language, perceive our world, and act autonomously, we must also grapple with the profound ethical implications of this technology.","sidebar":"mainSidebar"},"module-4-vla/intro-vla":{"id":"module-4-vla/intro-vla","title":"Introduction to Vision-Language-Action (VLA) Models","description":"Welcome to the frontier of AI-powered robotics! Modules 1, 2, and 3 have given us the building blocks for creating a robot: understanding the ROS 2 framework, simulating the robot in a digital twin, and using an AI brain like NVIDIA Isaac for perception and navigation.","sidebar":"mainSidebar"},"module-4-vla/natural-language-understanding":{"id":"module-4-vla/natural-language-understanding","title":"Natural Language Understanding for Robotics","description":"The \\"Language\\" part of a Vision-Language-Action (VLA) model is what allows the robot to understand human commands. This is achieved through a field of AI called Natural Language Understanding (NLU).","sidebar":"mainSidebar"},"module-4-vla/vla-frameworks-apis":{"id":"module-4-vla/vla-frameworks-apis","title":"Frameworks and APIs for Vision-Language-Action Models","description":"Building and deploying a Vision-Language-Action (VLA) model is a complex undertaking that requires a sophisticated stack of software tools. In this section, we will provide an overview of the key frameworks and APIs that are driving the development of VLA models.","sidebar":"mainSidebar"}}}}')}}]);