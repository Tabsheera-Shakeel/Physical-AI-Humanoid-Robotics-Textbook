"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[1125],{8453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>s});var t=o(6540);const i={},a=t.createContext(i);function r(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(a.Provider,{value:n},e.children)}},9125:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"module-4-vla/intro-vla","title":"Introduction to Vision-Language-Action (VLA) Models","description":"Welcome to the frontier of AI-powered robotics! Modules 1, 2, and 3 have given us the building blocks for creating a robot: understanding the ROS 2 framework, simulating the robot in a digital twin, and using an AI brain like NVIDIA Isaac for perception and navigation.","source":"@site/docs/module-4-vla/intro-vla.mdx","sourceDirName":"module-4-vla","slug":"/module-4-vla/intro-vla","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/module-4-vla/intro-vla","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"sidebar_label":"Introduction to VLA"},"sidebar":"mainSidebar","previous":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/category/module-4-vision-language-action-vla"},"next":{"title":"Natural Language Understanding","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/module-4-vla/natural-language-understanding"}}');var i=o(4848),a=o(8453);const r={sidebar_label:"Introduction to VLA"},s="Introduction to Vision-Language-Action (VLA) Models",l={},d=[{value:"What are VLA Models?",id:"what-are-vla-models",level:2},{value:"Why are VLAs Important for Robotics?",id:"why-are-vlas-important-for-robotics",level:2},{value:"How do VLAs Work?",id:"how-do-vlas-work",level:2}];function c(e){const n={h1:"h1",h2:"h2",header:"header",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"introduction-to-vision-language-action-vla-models",children:"Introduction to Vision-Language-Action (VLA) Models"})}),"\n",(0,i.jsx)(n.p,{children:"Welcome to the frontier of AI-powered robotics! Modules 1, 2, and 3 have given us the building blocks for creating a robot: understanding the ROS 2 framework, simulating the robot in a digital twin, and using an AI brain like NVIDIA Isaac for perception and navigation."}),"\n",(0,i.jsxs)(n.p,{children:["Now, we will explore how to give our robot a higher level of intelligence, allowing it to understand human language, perceive the world in a more holistic way, and take actions to achieve complex goals. This is the realm of ",(0,i.jsx)(n.strong,{children:"Vision-Language-Action (VLA) models"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"what-are-vla-models",children:"What are VLA Models?"}),"\n",(0,i.jsx)(n.p,{children:"A Vision-Language-Action model is a type of AI model that can take input from multiple modalities (vision, language) and produce an action as output."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Vision:"}),' The model can "see" the world through a camera, understanding the objects, their relationships, and the overall scene.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Language:"}),' The model can understand natural language commands from a human, such as "pick up the red apple" or "drive to the kitchen".']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action:"})," The model can generate a sequence of actions to achieve the goal described in the language command, based on its visual understanding of the environment."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://www.researchgate.net/profile/Cheng-Chi-Wang-3/publication/365903395/figure/fig1/AS:11431281143358129@1672403443931/An-illustration-of-our-vision-language-action-VLA-model-for-robot-manipulation-The.ppm",alt:"A diagram illustrating the VLA concept: Vision and Language as inputs to a model that produces an Action as output."})}),"\n",(0,i.jsx)(n.h2,{id:"why-are-vlas-important-for-robotics",children:"Why are VLAs Important for Robotics?"}),"\n",(0,i.jsx)(n.p,{children:"VLA models represent a paradigm shift in how we interact with and control robots. Instead of writing complex, low-level code to program a robot for a specific task, we can simply tell the robot what we want it to do in plain English."}),"\n",(0,i.jsx)(n.p,{children:"This has several profound implications:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Accessibility:"})," It makes robotics accessible to non-experts. Anyone can control a robot without needing to be a programmer."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Flexibility:"})," The robot is no longer limited to a predefined set of tasks. It can generalize its knowledge to perform new tasks in new environments."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Robustness:"})," By grounding language in vision, the robot can better understand the context of a command and recover from errors."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"how-do-vlas-work",children:"How do VLAs Work?"}),"\n",(0,i.jsxs)(n.p,{children:["At a high level, VLA models are typically based on the ",(0,i.jsx)(n.strong,{children:"Transformer architecture"}),", which is the same architecture that powers Large Language Models (LLMs) like GPT-3."]}),"\n",(0,i.jsx)(n.p,{children:"The model is trained on a massive dataset of robot experiences, which includes:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'Camera images (the "vision").'}),"\n",(0,i.jsx)(n.li,{children:"Language commands that were given to the robot."}),"\n",(0,i.jsx)(n.li,{children:"The sequence of actions the robot took to complete the command."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:'By learning the correlations between these three modalities, the model learns a "policy" that maps from a given language command and a visual input to a sequence of actions.'}),"\n",(0,i.jsx)(n.p,{children:"In the following sections, we will delve deeper into the key components of VLA models: natural language understanding, action generation, and the frameworks and APIs that are making this technology a reality."})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}}}]);