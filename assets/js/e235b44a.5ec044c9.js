"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[4885],{1506:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-4-vla/natural-language-understanding","title":"Natural Language Understanding for Robotics","description":"The \\"Language\\" part of a Vision-Language-Action (VLA) model is what allows the robot to understand human commands. This is achieved through a field of AI called Natural Language Understanding (NLU).","source":"@site/docs/module-4-vla/natural-language-understanding.mdx","sourceDirName":"module-4-vla","slug":"/module-4-vla/natural-language-understanding","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module-4-vla/natural-language-understanding","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"sidebar_label":"Natural Language Understanding"},"sidebar":"mainSidebar","previous":{"title":"Introduction to VLA","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module-4-vla/intro-vla"},"next":{"title":"Action Generation","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module-4-vla/action-generation"}}');var a=t(4848),o=t(8453);const r={sidebar_label:"Natural Language Understanding"},s="Natural Language Understanding for Robotics",l={},d=[{value:"The Role of Large Language Models (LLMs)",id:"the-role-of-large-language-models-llms",level:2},{value:"The NLU Pipeline: Intent and Entity Recognition",id:"the-nlu-pipeline-intent-and-entity-recognition",level:2},{value:"Example: A Simple NLU Pipeline in Python",id:"example-a-simple-nlu-pipeline-in-python",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"natural-language-understanding-for-robotics",children:"Natural Language Understanding for Robotics"})}),"\n",(0,a.jsxs)(n.p,{children:['The "Language" part of a Vision-Language-Action (VLA) model is what allows the robot to understand human commands. This is achieved through a field of AI called ',(0,a.jsx)(n.strong,{children:"Natural Language Understanding (NLU)"}),"."]}),"\n",(0,a.jsx)(n.p,{children:'NLU is focused on enabling computers to understand the meaning and intent behind human language. In the context of robotics, NLU is used to parse a command like "bring me the red ball from the table" and extract the key information that the robot needs to act upon.'}),"\n",(0,a.jsx)(n.h2,{id:"the-role-of-large-language-models-llms",children:"The Role of Large Language Models (LLMs)"}),"\n",(0,a.jsx)(n.p,{children:"The recent advancements in Large Language Models (LLMs), such as GPT-3, BERT, and T5, have revolutionized the field of NLU. These models are trained on vast amounts of text data and have a deep understanding of grammar, syntax, and semantics."}),"\n",(0,a.jsx)(n.p,{children:"In robotics, we can leverage the power of LLMs in several ways:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Zero-Shot Command Parsing:"})," For simple commands, a pre-trained LLM can often extract the key information without any additional training."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Few-Shot Learning:"})," By providing the LLM with just a few examples of commands and their desired outputs, we can quickly adapt the model to a specific robotics domain."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Fine-Tuning:"})," For more complex applications, we can fine-tune a pre-trained LLM on a dataset of robotics-specific commands to achieve very high accuracy."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"the-nlu-pipeline-intent-and-entity-recognition",children:"The NLU Pipeline: Intent and Entity Recognition"}),"\n",(0,a.jsx)(n.p,{children:"A typical NLU pipeline for robotics involves two main steps:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Intent Recognition:"}),' This is the task of identifying the user\'s intention. For example, in the command "bring me the red ball", the intent is ',(0,a.jsx)(n.code,{children:"bring"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Entity Recognition:"})," This is the task of extracting the key entities from the command. In our example, the entities are ",(0,a.jsx)(n.code,{children:"red ball"})," (the object to be brought) and ",(0,a.jsx)(n.code,{children:"table"})," (the location of the object)."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"The output of the NLU pipeline is a structured representation of the command that can be easily understood by the robot's decision-making system."}),"\n",(0,a.jsx)(n.h2,{id:"example-a-simple-nlu-pipeline-in-python",children:"Example: A Simple NLU Pipeline in Python"}),"\n",(0,a.jsxs)(n.p,{children:["Let's look at a simplified example of how you might use a pre-trained model from a library like ",(0,a.jsx)(n.a,{href:"https://huggingface.co/transformers/",children:"Hugging Face Transformers"})," to perform NLU. This example will use a simple form of entity recognition."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from transformers import pipeline\n\n# Load a pre-trained model for Named Entity Recognition (NER)\nner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n\ncommand = \"Please bring me the red ball from the table\"\n\n# Run the command through the NER pipeline\nentities = ner_pipeline(command)\n\n# Process the entities to extract the relevant information\nobject_to_bring = None\nlocation = None\n\nfor entity in entities:\n    if entity['entity_group'] == 'MISC': # Miscellaneous entity\n        if 'ball' in entity['word']:\n            object_to_bring = entity['word']\n    elif entity['entity_group'] == 'LOC': # Location entity\n        if 'table' in entity['word']:\n            location = entity['word']\n\nprint(f\"Intent: bring (inferred)\")\nprint(f\"Object: {object_to_bring}\")\nprint(f\"Location: {location}\")\n"})}),"\n",(0,a.jsx)(n.p,{children:"This is a very basic example, and a real-world robotics NLU system would be more sophisticated. It might involve:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"A custom-trained model for intent and entity recognition."}),"\n",(0,a.jsx)(n.li,{children:"Handling of more complex sentence structures."}),"\n",(0,a.jsx)(n.li,{children:"Integration with a dialogue management system to ask for clarification if the command is ambiguous."}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"However, this example illustrates the basic principle of using a pre-trained language model to extract structured information from a natural language command. This structured information is then passed to the action generation part of the VLA model, which we will discuss in the next section."})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var i=t(6540);const a={},o=i.createContext(a);function r(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);