---
sidebar_position: 4
sidebar_label: Glossary
title: Glossary
---

# Glossary

This glossary provides definitions for key terms and acronyms used throughout this book on Physical AI Humanoids.

## A

**Action:** In ROS 2, a long-running goal-feedback-result communication mechanism. In VLA models, the physical behaviors or commands a robot executes.

**Actuator:** A component of a robot that is responsible for moving and controlling a mechanism or system (e.g., motor, servo).

**AI (Artificial Intelligence):** The simulation of human intelligence in machines that are programmed to think like humans and mimic their actions.

**AMCL (Adaptive Monte Carlo Localization):** A probabilistic localization algorithm used in ROS 2's Nav2 stack.

**API (Application Programming Interface):** A set of defined methods of communication among various components.

**Ament:** The build system used by ROS 2.

**Autonomous Robot:** A robot that can perform tasks with a high degree of independence from human intervention.

## C

**CIM (Computational Intelligence Module):** A general term sometimes used for the "brain" or core processing unit of a robot.

**Codelet:** A fundamental execution unit in the NVIDIA Isaac SDK.

**Colcon:** A command-line tool used to build, test, and install multiple software packages in ROS 2 workspaces.

**Collision Geometry:** The simplified 3D shape used by a physics engine to detect collisions between objects, typically distinct from visual geometry.

**Computer Vision:** A field of AI that enables computers and systems to derive meaningful information from digital images, videos and other visual inputs.

## D

**DDS (Data Distribution Service):** A middleware standard for real-time systems that ROS 2 is built upon, handling data transport.

**Digital Twin:** A virtual replica of a physical object, process, or system that serves as an exact real-time digital counterpart.

**Dof (Degrees of Freedom):** The number of independent parameters that define the configuration of a mechanical system.

## E

**Edge AI:** AI processing performed locally on a device (e.g., a robot) rather than in the cloud.

**End-Effector:** The device or tool attached to the end of a robot arm, designed to interact with the environment (e.g., gripper, hand, tool).

## G

**Gazebo:** A popular 3D dynamic robot simulator compatible with ROS 2.

**GEM (Graph Enabled Module):** A reusable, GPU-accelerated software component in the NVIDIA Isaac SDK.

**GPU (Graphics Processing Unit):** A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images, crucial for AI computations.

**Ground Truth:** Data that is known to be accurate, typically collected from the real world, used to train and evaluate models.

## H

**HRI (Human-Robot Interaction):** The study of how humans and robots can interact with each other.

## I

**IK (Inverse Kinematics):** The process of calculating the joint parameters that are required to achieve a desired end-effector pose.

**IMU (Inertial Measurement Unit):** An electronic device that measures and reports a body's specific force, angular rate, and sometimes the magnetic field surrounding the body, using a combination of accelerometers, gyroscopes, and magnetometers.

**Inertial Properties:** Physical characteristics of an object related to its mass and resistance to changes in motion (e.g., mass, center of mass, inertia tensor).

**Isaac SDK:** NVIDIA's software development kit for AI-powered robots, providing tools and GEMs.

**Isaac Sim:** NVIDIA's robotics simulation application built on the Omniverse platform.

## J

**Jetson:** NVIDIA's series of embedded computing boards designed for AI at the edge.

**Joint:** A connection between two links in a robot, allowing relative motion.

**Joint State:** The current position, velocity, and effort of a robot's joints.

## L

**Language Grounding:** The process of connecting linguistic symbols and expressions to perceptions and actions in the physical world.

**LiDAR (Light Detection and Ranging):** A remote sensing method that uses light in the form of a pulsed laser to measure ranges (variable distances) to the Earth.

**Link:** A rigid body segment of a robot in a kinematic chain.

**LLM (Large Language Model):** A type of artificial intelligence program that can recognize and generate text and other content based on massive datasets.

**Localization:** The process of a robot determining its own position and orientation within a known map.

## M

**Manipulation:** The task of a robot interacting with objects in its environment, typically involving grasping, moving, or assembling.

**MDX:** Markdown with JSX, allowing for interactive components within Markdown documents.

**Message:** A data structure used for communication in ROS 2.

**MoveIt 2:** A powerful motion planning framework for ROS 2.

## N

**Nav2 (ROS 2 Navigation Stack):** A collection of ROS 2 packages that enable a robot to autonomously navigate from a starting position to a goal position.

**NLU (Natural Language Understanding):** A subfield of AI that focuses on enabling computers to understand and interpret human language.

**Node:** An executable that uses ROS 2 to communicate with other nodes; a fundamental building block of a ROS 2 system.

## O

**Object Detection:** The task of identifying and locating objects within an image or video.

**Omniverse:** NVIDIA's platform for connecting and building custom 3D pipelines and simulating large-scale virtual worlds.

## P

**Package:** The fundamental unit of software organization in ROS 2.

**Parameter:** A configuration value for a ROS 2 node that can be changed at runtime.

**Physical AI:** The convergence of artificial intelligence with real-world physical systems, enabling intelligent agents to perceive, reason, and act within the physical world.

**Pose:** The position and orientation of an object or robot in 3D space.

## R

**Reinforcement Learning (RL):** A type of machine learning where an agent learns to make decisions by performing actions in an environment to maximize a cumulative reward.

**RMW (ROS Middleware):** The layer in ROS 2 responsible for communication between processes, using DDS.

**Robot Description Format:** A standardized way to describe the physical and kinematic properties of a robot (e.g., URDF, SDF).

**ROS 2 (Robot Operating System 2):** A flexible framework for writing robot software.

**`ros2_control`:** The primary framework for hardware abstraction in ROS 2.

**rviz2:** A 3D visualization tool for ROS 2.

## S

**SDF (Simulation Description Format):** An XML format for describing objects and environments for robot simulators, especially Gazebo.

**Sensor:** A device that detects or measures a physical property and records, indicates, or otherwise responds to it (e.g., camera, LiDAR, IMU).

**Service:** In ROS 2, a synchronous request-response communication mechanism between nodes.

**Sim-to-Real Gap:** The discrepancy between the performance of a robot in simulation versus its performance in the real world.

**SLAM (Simultaneous Localization and Mapping):** The computational problem of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location within it.

**Synthetic Data Generation:** The process of creating artificial data, often using simulations, to train machine learning models.

## T

**TensorRT:** NVIDIA's SDK for high-performance deep learning inference.

**Topic:** In ROS 2, an asynchronous one-to-many publish-subscribe communication mechanism.

## U

**Unity3D:** A popular game development platform also used for high-fidelity robotics simulation.

**URDF (Unified Robot Description Format):** An XML file format in ROS to describe all elements of a robot.

**USD (Universal Scene Description):** A 3D scene description technology developed by Pixar, forming the foundation of NVIDIA Omniverse.

## V

**VIO (Visual-Inertial Odometry):** A method for estimating the pose of an agent by combining visual (camera) and inertial (IMU) sensor data.

**Vision-Language-Action (VLA):** A class of AI models that integrates visual perception, natural language understanding, and physical action generation.

## X

**Xacro (XML Macros for ROS):** An XML macro language that allows you to use macros, variables, and mathematical expressions to generate URDF files more efficiently.

**ZMP (Zero Moment Point):** A concept in robotics and biomechanics that is used to describe the point on the ground where the net moment of all active forces (gravity and inertia) equals zero, crucial for bipedal balance.